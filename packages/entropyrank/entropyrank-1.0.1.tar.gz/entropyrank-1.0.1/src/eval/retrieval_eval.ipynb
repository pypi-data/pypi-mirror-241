{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "from transformers import AutoTokenizer, GPTNeoForCausalLM\n",
    "\n",
    "\n",
    "class ConditionalEntropyRanker:\n",
    "    def __init__(self, model=None, tokenizer=None, device=None):\n",
    "        self.device = (\n",
    "            device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "        self.tokenizer = (\n",
    "            AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\") if not tokenizer else tokenizer\n",
    "        )\n",
    "        self.model = (\n",
    "            GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\") if not model else model\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.nlp.max_length = 10000000\n",
    "\n",
    "    def get_output_entropy(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output: str,\n",
    "        exclude_start_words_count=0,\n",
    "    ):\n",
    "        text = prompt + \"\\n\" + output\n",
    "        normalized_text = ConditionalEntropyRanker._normalize_text(text)\n",
    "        normalized_prompt = ConditionalEntropyRanker._normalize_text(prompt)\n",
    "        tokenized_text = self._tokenize_text(normalized_text)\n",
    "        tokenized_prompt = self._tokenize_text(normalized_prompt)\n",
    "        word_to_token_indices_prompt = self._map_words_to_token_indices(tokenized_prompt)\n",
    "\n",
    "        words_to_tokens_indices = self._map_words_to_token_indices(tokenized_text)\n",
    "        entropy, relevant_tokens, _ = self._get_tokens_entropy(tokenized_text)\n",
    "        decoded = self.tokenizer.batch_decode(relevant_tokens)\n",
    "        original_words = ConditionalEntropyRanker._get_original_words(\n",
    "            decoded, words_to_tokens_indices\n",
    "        )\n",
    "\n",
    "        original_words, entropy = self._get_text_entropy(\n",
    "            original_words=original_words,\n",
    "            entropy=entropy,\n",
    "            words_to_tokens_indices=words_to_tokens_indices,\n",
    "            exclude_start_words_count=exclude_start_words_count,\n",
    "        )\n",
    "        # skip len(word_to_token_indices_prompt) because we don't want to include the prompt in the entropy\n",
    "        relevant_words = original_words[len(word_to_token_indices_prompt) + 1 :]\n",
    "        relevant_entropy = entropy[len(word_to_token_indices_prompt) + 1 :]\n",
    "\n",
    "        # Concat word to the previous word if it's \"'s\"\n",
    "        (\n",
    "            relevant_words,\n",
    "            relevant_entropy,\n",
    "        ) = ConditionalEntropyRanker._concat_apostrophes_to_previous_word(\n",
    "            relevant_words, relevant_entropy\n",
    "        )\n",
    "        return list(zip(relevant_words, relevant_entropy))\n",
    "\n",
    "    def get_relevance_score(self, query: str, passage: str, exclude_start_words_count=0):\n",
    "        output = self.get_output_entropy(\n",
    "            prompt=query, output=passage, exclude_start_words_count=exclude_start_words_count\n",
    "        )\n",
    "        entropy = [x[1] for x in output]\n",
    "        return sum(entropy) / len(entropy)\n",
    "\n",
    "    def _get_text_entropy(\n",
    "        self,\n",
    "        original_words: list[str],\n",
    "        entropy: torch.Tensor,\n",
    "        words_to_tokens_indices: list[tuple[int, int]],\n",
    "        exclude_start_words_count=0,\n",
    "    ):\n",
    "        entropy = ConditionalEntropyRanker._get_entropy_of_original_words(\n",
    "            original_words, entropy, words_to_tokens_indices\n",
    "        )\n",
    "        return original_words, entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_original_words(\n",
    "        decoded: list[str], words_to_tokens_indices: list[tuple[int, int]]\n",
    "    ) -> list[str]:\n",
    "        original_words = [\n",
    "            \"\".join(decoded[start:end]).strip() for start, end in words_to_tokens_indices\n",
    "        ]\n",
    "        return original_words\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_tokens_entropy(\n",
    "        self, tokenized_text: str\n",
    "    ) -> tuple[torch.Tensor, list[int], torch.Tensor]:\n",
    "        # convert tokenized text to gpu tensors\n",
    "        tokenized_text = {k: v.to(self.device) for k, v in tokenized_text.items()}\n",
    "        # Get probabilities from logits\n",
    "        logits = self.model(**tokenized_text).logits\n",
    "\n",
    "        # Remove bos token from tokenized email\n",
    "        relevant_tokens = tokenized_text[\"input_ids\"][0][1:]\n",
    "\n",
    "        entropy = ConditionalEntropyRanker._calculate_entropy_from_logits(logits)\n",
    "\n",
    "        return entropy, relevant_tokens, logits\n",
    "\n",
    "    def _tokenize_text(self, text: str):\n",
    "        tokenized_text = self.tokenizer(\n",
    "            self.tokenizer.bos_token + text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        return tokenized_text\n",
    "\n",
    "    @staticmethod\n",
    "    def _concat_apostrophes_to_previous_word(\n",
    "        token_strings: list[str], entropies: list[float]\n",
    "    ) -> tuple[list[str], list[float]]:\n",
    "        new_token_strings = []\n",
    "        new_entropies = []\n",
    "\n",
    "        if len(token_strings) != len(entropies):\n",
    "            return token_strings, entropies\n",
    "        # Concatenate apostrophes to previous word\n",
    "        for i in range(len(token_strings)):\n",
    "            if token_strings[i] == \"'s\" or token_strings[i] == \"'t\":\n",
    "                new_token_strings[-1] += token_strings[i]\n",
    "                new_entropies[-1] += entropies[i]\n",
    "            else:\n",
    "                new_token_strings.append(token_strings[i])\n",
    "                new_entropies.append(entropies[i])\n",
    "\n",
    "        return new_token_strings, new_entropies\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_entropy_of_original_words(\n",
    "        original_words: list[str],\n",
    "        entropy: torch.Tensor,\n",
    "        words_to_token_indices: list[tuple[int, int]],\n",
    "    ) -> list[float]:\n",
    "        mapped_entropy = []\n",
    "        if len(original_words) != len(words_to_token_indices):\n",
    "            print(\"Error in mapping entropy to original words\")\n",
    "        for start, end in words_to_token_indices:\n",
    "            mapped_entropy.append(entropy[start:end].sum().item())\n",
    "\n",
    "        return mapped_entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_text(text: str) -> str:\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def _calculate_entropy_from_logits(logits: torch.Tensor) -> torch.Tensor:\n",
    "        # make these calculations on the cpu\n",
    "        probs = logits[0].softmax(dim=-1)\n",
    "        # We don't need prediction for the last token\n",
    "        relevant_probs = probs[range(len(probs) - 1), :]\n",
    "        entropy = -torch.sum(relevant_probs * torch.log2(relevant_probs), dim=-1)\n",
    "\n",
    "        # check if there is nan here - this happens when the probability is really close to 0\n",
    "        if torch.isnan(entropy).any():\n",
    "            # replace any nan with 0\n",
    "            entropy[torch.isnan(entropy)] = 0\n",
    "\n",
    "        return entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def _map_words_to_token_indices(encoded) -> list[tuple[int, int]]:\n",
    "        desired_output = []\n",
    "        for word_id in encoded.word_ids():\n",
    "            if word_id is not None:\n",
    "                start, end = encoded.word_to_tokens(word_id)\n",
    "                # we subtract 1 from start because the first token is BOS token,\n",
    "                tokens = (start - 1, end - 1)\n",
    "                if len(desired_output) == 0 or desired_output[-1] != tokens:\n",
    "                    desired_output.append(tokens)\n",
    "\n",
    "        # remove first token index because it's BOS token\n",
    "        return desired_output[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin: 100%|██████████| 2.84G/2.84G [00:53<00:00, 53.0MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 69.0/69.0 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 237/237 [00:00<00:00, 236kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 1.50MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.43MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 3.18MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer = ConditionalEntropyRanker(model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(cer.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is the main purpose of alchemy?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "#     \"The Atlantic Ocean is the second largest ocean in the world, with an area of 31,830,000 sq mi (82,440,000 sq km) 12. It covers approximately 17% of Earth’s surface and about 24% of its water surface area 3. The Atlantic Ocean separates North and South America from Europe and Africa 1.\",\n",
    "#     \"Various networking Companies use 192.168.0.1 IP address to access their admin page. We solve issues related to this IP address such as 192.168.0.1 login my account issue, 192.168.0.1 the page cannot be displayed etc. Feel free to contact us through live chat window.\",\n",
    "#     \"It seems like their main problem is not hiring - clearly they've hired some bright technical people. It just seems like the iPhone and Android came along and management refused to admit that the new platforms were going to compete with the Blackberry and its ecosystem.\",\n",
    "#     \"In order to do this, you would need to purchase an Amazon gift card from another online vendor that accepts PayPal and then use the Amazon gift card on the Amazon site. There are dozens (if not hundreds) of sites that sell Amazon gift cards online that accept PayPal.\",\n",
    "#     \"So one approach would be purely mathematical: look at whichever has the higher interest rate and pay it first. Another approach is to ignore the math (since the interest savings difference between a mortgage and student loan is likely small anyways) and think about what your goals are. Do you like having a student loan payment? Would you prefer to get rid of it as quickly as possible? How would it feel to cut the balance in HALF in one shot? If it were me, I would pay the student loan as fast as possible. Student loans are not cancellable or bankruptable, and once you get it paid off you can put that payment amount toward your house to get it paid off.\",\n",
    "#     \"I used to work for one of the three ratings agencies. Awhile ago. First: There are lots of different ratings. The bulk of ratings are for corporate debt and public finance. So senior debentures (fixed income) and General Obligations e.g. tax-free muni bonds, respectively. Ratings agencies are NOT paid by the investment banks, they are paid by the corporations or city/ state that is issuing debt. The investment banks are the syndicate that pulls the transaction together and brings it to market. For mortgage-backed securities, collateralized debt CDO-CLO's, all of which are fancy structured securitizations, well, that is a different matter! Those transactions are the ones where there is an inappropriately close tie between the investment bankers and ratings agencies. And those were the ratings that blew out and caused problems. Ratings agencies continued to do a decent job with what WAS their traditional business, corporate and municipal bond ratings, as far as I know. What khajja said was 100% correct: S&amp;P's fees were paid by investors, the people who were purchasing the bonds, until about 50 years ago. Around the same time that McGraw-Hill purchased S&amp;P, in 1966, they departed from that model, and started charging the bond issuers for ratings. I don't know if that decision was driven by McGraw-Hill or not, though. One more thing: Not all credit ratings agencies are paid by the issuers. One of the 10 NRSRO's (a designation given by the S.E.C.) is Egan-Jones. Their revenue comes from the investors, bond purchasers, not the companies issuing bonds, unlike the S&amp;P/ Fitch/ Moody's \"\"business model\"\". So there is an alternative, which I consider hopeful and reason not to totally despair. EDIT: What xcrunna19 mentions is also totally accurate. The part about Nouriel Roubini (who is a professor at N.Y.U. or Columbia or such and a sensible though slightly high strung sort) is consistent with my impression. As for whether it would require government action to implement the changes advocated by Roubini, yes, I guess it would, but I don't know if the government would do that. It would be better if the credit ratings agencies would find their own way to a different, less conflicted payment-incentive model. Keep in mind too that many of the provisions of Dodd-Frank have removed the existing regulatory requirements for credit ratings on bonds and other securities. This is the scary part though: There isn't anything to replace the credit ratings agencies, not at the moment, as far as I can tell! Eventually the government is supposed to come up with an alternative, but that hasn't happened yet. Which is better: Not requiring ratings at all, or the past situation of sometimes inflated ratings, which imparted a false sense of confidence? I don't know.\",\n",
    "#     \"You will be filing the exact same form you've been filing until now (I hope...) which is called form 1040. Attached to it, you'll add a \"\"Schedule C\"\" form and \"\"Schedule SE\"\" form. Keep in mind the potential effect of the tax and totalization treaties the US has with the UK which may affect your filings. I suggest you talk to a licensed EA/CPA who works with expats in the UK and is familiar with all the issues. There are several prominent offices you can find by Googling.\",\n",
    "#     \"First, to answer the question. The benefit of a 401k is that you don't have to pay income tax on the money contributed nor do you pay capital gains tax on the money that accumulates. You get that with the restriction that you can't willy nilly remove and contribute money to the account (and you are taxed on withdrawals, more severely if you do it before you are 65). Similar sorts of restrictions apply to all retirement accounts which give tax benefits. Now, for the 7000 not providing benefit. Assuming a very modest 4% growth, over 40 years 7000 becomes 34,671. Not something to sneeze at (inflation, risk reward, blah, blah, blah, it is less than it looks, but 4% is really pretty low, the stock market averages anywhere from 7-&gt;10% and IIRC the bond market is somewhere around 5%). Now, certainly, to avoid bankruptcy you should withdraw. However, if it is possible, you will be best served by keeping the money in your 401k account. The penalties and lost earning opportunities are pretty significant. /u/BeatArmy99 [has the numbers](http://www.reddit.com/r/finance/comments/2ct0qy/why_cant_i_access_my_401k_if_its_my_money/cjiorl7) for how much you lose by doing an early withdraw. Don't do this lightly and I would suggest avoiding cashing out the whole thing if you can.\",\n",
    "#     \"I can only speak to natural gas but I imagine the answer for electricity is the same. In general, yes, it is better to lock into a fixed price contract as in the long run, natural gas prices increase over time. However, if you locked (signed a fixed price contract) in prior to the economic downturn, most likely you were better off not doing so but the key is long-term. http://en.wikipedia.org/wiki/Natural_gas_prices However, do your research as fixed priced contracts vary considerably from company to company. http://www.energyshop.com/ I think it's a good time to sign a fixed-term contract right now as I don't see prices coming down much further with global economies are now recovering from the downturn. HTH\",\n",
    "#     \"Name one nation state that has survived more than 20 minutes without taxation. People won't pay if they don't have to, things don't get built if people don't pay. Take a holiday to Somalia if you want to see a libertarian paradise in action.\",\n",
    "# \"An alphabet is a standard set of letters (basic written symbols or graphemes) that is used to write one or more languages based upon the general principle that the letters represent phonemes (basic significant sounds) of the spoken language. This is in contrast to other types of writing systems, such as syllabaries (in which each character represents a syllable) and logographies (in which each character represents a word, morpheme, or semantic unit).\t\",\n",
    "\"Answer: An android is a humanoid robot or synthetic organism designed to look and act like a human, especially one with a body having a flesh-like resemblance. Historically, androids remained completely within the domain of science fiction where they are frequently seen in film and television. Only recently have advancements in robot technology allowed the design of functional and realistic humanoid robots.\",\n",
    "# \"\"\"Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical physicist. Einstein developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).\"\"\",\n",
    "\"Answer: It is a brief statement that contains the most important points of a long legal document or of several related legal papers.\",\n",
    "\"\"\"Answer: It aimed to purify, mature, and perfect certain objects. Common aims were chrysopoeia, the transmutation of \"base metals\" (e.g., lead) into \"noble metals\" (particularly gold); the creation of an elixir of immortality; the creation of panaceas able to cure any disease; and the development of an alkahest, a universal solvent. The perfection of the human body and soul was thought to permit or result from the alchemical magnum opus and, in the Hellenistic and western tradition, the achievement of gnosis. In Europe, the creation of a philosopher's stone was variously connected with all of these projects.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.010985343131636, 2.8700875712931158, 3.2118490908092623]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# output=cer.get_relevance_score(query=\"What is the second largest ocean?\"\n",
    "#                               ,passage=\"\"\"In law, an abstract is a brief statement that contains the most important points of a long legal document or of several related legal papers.\n",
    "# \"\"\")\n",
    "# get scores for all documents\n",
    "scores = []\n",
    "for doc in documents:\n",
    "    scores.append(cer.get_relevance_score(query=query, passage=doc))\n",
    "\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.088653205374283, 3.5551322285085916, 4.341659794960703, 2.895300578394974, 3.4035164560708733, 4.161020574215866, 4.262144573032856, 3.958267181936525, 3.616004661907657, 3.68802069616504, 2.7226829525576357, 3.100113259080578, 1.9204128443812714, 3.5143199951752373, 3.2988936337987305]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'beir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tsale\\OneDrive\\Desktop\\CS Masters Degree\\EntropyRank\\src\\eval\\retrieval_eval.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tsale/OneDrive/Desktop/CS%20Masters%20Degree/EntropyRank/src/eval/retrieval_eval.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbeir\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreranking\u001b[39;00m \u001b[39mimport\u001b[39;00m Rerank\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsale/OneDrive/Desktop/CS%20Masters%20Degree/EntropyRank/src/eval/retrieval_eval.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mYourCustomCEModel\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tsale/OneDrive/Desktop/CS%20Masters%20Degree/EntropyRank/src/eval/retrieval_eval.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'beir'"
     ]
    }
   ],
   "source": [
    "from beir.reranking import Rerank\n",
    "\n",
    "class YourCustomCEModel:\n",
    "    def __init__(self, model_path=None, **kwargs):\n",
    "        self.model = None # ---> HERE Load your custom model\n",
    "    \n",
    "    # Write your own score function, which takes in query-document text pairs and returns the similarity scores\n",
    "    def predict(self, sentences: List[Tuple[str,str]], batch_size: int, **kwags) -> List[float]:\n",
    "        pass # return only the list of float scores\n",
    "\n",
    "reranker = Rerank(YourCustomCEModel(model_path=\"your-custom-model-path\"), batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

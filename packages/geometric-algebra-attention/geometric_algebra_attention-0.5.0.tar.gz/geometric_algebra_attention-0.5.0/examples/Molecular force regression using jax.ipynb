{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/klarh/geometric_algebra_attention/blob/master/examples/Molecular%20force%20regression%20using%20jax.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Colab-specific setup that will be ignored elsewhere\n",
    "if [ ! -z \"$COLAB_GPU\" ]; then\n",
    "    pip install flowws-keras-geometry flowws-keras-experimental\n",
    "    pip install git+https://github.com/klarh/geometric_algebra_attention\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More TPU and colab-specific setup\n",
    "import os\n",
    "if 'TPU_NAME' in os.environ:\n",
    "    import jax.tools.colab_tpu\n",
    "    jax.tools.colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws_keras_geometry.data import MD17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "\n",
    "class ClearMetrics(flowws.Stage):\n",
    "    \"\"\"Clear model metrics, for example the keras-based ones that MD17 defines\"\"\"\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        scope.pop('metrics', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental.stax import serial, Dense\n",
    "\n",
    "from geometric_algebra_attention.jax import VectorAttention\n",
    "\n",
    "def make_layernorm():\n",
    "    def init(rng, input_shape):\n",
    "        return input_shape, ()\n",
    "\n",
    "    def eval_(params, x, rng=None):\n",
    "        return jax.nn.normalize(x)\n",
    "\n",
    "    return init, eval_\n",
    "\n",
    "def make_pairwise_difference():\n",
    "    def init(rng, input_shape):\n",
    "        result = list(input_shape)\n",
    "        result.insert(-2, result[-2])\n",
    "        return tuple(result), ()\n",
    "\n",
    "    def eval_(params, x, rng=None):\n",
    "        return x[..., None, :] - x[..., None, :, :]\n",
    "\n",
    "    return init, eval_\n",
    "\n",
    "def make_pairwise_sum_difference():\n",
    "    def init(rng, input_shape):\n",
    "        result = list(input_shape)\n",
    "        result.insert(-2, result[-2])\n",
    "        result[-1] *= 2\n",
    "        return tuple(result), ()\n",
    "\n",
    "    def eval_(params, x, rng=None):\n",
    "        minus = x[..., None, :] - x[..., None, :, :]\n",
    "        plus = x[..., None, :] - x[..., None, :, :]\n",
    "        return jnp.concatenate([minus, plus], axis=-1)\n",
    "\n",
    "    return init, eval_\n",
    "\n",
    "def make_swish():\n",
    "    def init(rng, input_shape):\n",
    "        return input_shape, ()\n",
    "\n",
    "    def eval_(params, x, rng=None):\n",
    "        return jax.nn.swish(x)\n",
    "\n",
    "    return init, eval_\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class MoleculeForceRegression(flowws.Stage):\n",
    "    \"\"\"Build a geometric algebra attention network for a molecular force regression task.\n",
    "    \"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('rank', None, int, 2,\n",
    "            help='Degree of correlations (n-vectors) to consider'),\n",
    "        Arg('n_dim', '-n', int, 32,\n",
    "            help='Working dimensionality of point representations'),\n",
    "        Arg('dilation', None, float, 2,\n",
    "            help='Working dimension dilation factor for MLP components'),\n",
    "        Arg('merge_fun', '-m', str, 'concat',\n",
    "            help='Method to merge point representations'),\n",
    "        Arg('join_fun', '-j', str, 'concat',\n",
    "            help='Method to join invariant and point representations'),\n",
    "        Arg('n_blocks', '-b', int, 2,\n",
    "            help='Number of deep blocks to use'),\n",
    "        Arg('block_nonlinearity', None, bool, True,\n",
    "            help='If True, add a nonlinearity to the end of each block'),\n",
    "        Arg('residual', '-r', bool, True,\n",
    "            help='If True, use residual connections within blocks'),\n",
    "        Arg('invariant_mode', None, str, 'single',\n",
    "            help='Attention mechanism rotation-invariant attribute mode'),\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        rank = self.arguments['rank']\n",
    "        n_dim = self.arguments['n_dim']\n",
    "        dilation_dim = int(np.round(n_dim*self.arguments['dilation']))\n",
    "        merge_fun = self.arguments['merge_fun']\n",
    "        join_fun = self.arguments['join_fun']\n",
    "        invar_mode = self.arguments['invariant_mode']\n",
    "\n",
    "        score = serial(\n",
    "            Dense(dilation_dim),\n",
    "            make_swish(),\n",
    "            Dense(1)\n",
    "            )\n",
    "\n",
    "        value = serial(\n",
    "            Dense(dilation_dim),\n",
    "            make_layernorm(),\n",
    "            make_swish(),\n",
    "            Dense(n_dim)\n",
    "            )\n",
    "\n",
    "        rij_layer = make_pairwise_difference()\n",
    "        vij_layer = make_pairwise_sum_difference()\n",
    "\n",
    "        def make_attention(reduce=False):\n",
    "            attention = VectorAttention(\n",
    "                score, value, reduce=reduce, rank=rank, merge_fun=merge_fun,\n",
    "                join_fun=join_fun, invariant_mode=invar_mode).stax_functions\n",
    "            return attention\n",
    "\n",
    "        def init(rng, input_shape):\n",
    "            (r_shape, v_shape) = input_shape\n",
    "\n",
    "            def rngs_(rng):\n",
    "                while True:\n",
    "                    (next_rng, rng) = jax.random.split(rng)\n",
    "                    yield next_rng\n",
    "            rngs = rngs_(rng)\n",
    "\n",
    "            def param(layer, sh):\n",
    "                (last_shape, p) = layer[0](next(rngs), sh)\n",
    "                params.append(p)\n",
    "                return last_shape\n",
    "\n",
    "            params = []\n",
    "\n",
    "            r_shape = param(rij_layer, r_shape)\n",
    "            v_shape = param(vij_layer, v_shape)\n",
    "\n",
    "            last_shape = param(vscale, v_shape)\n",
    "            for i, att in enumerate(attentions):\n",
    "                last_shape = param(att, (r_shape, last_shape))\n",
    "                if self.arguments['block_nonlinearity']:\n",
    "                    last_shape = param(block_nonlins[i], last_shape)\n",
    "            last_shape = param(final_attention, (r_shape, last_shape))\n",
    "            last_shape = param(final_mlp, last_shape)\n",
    "\n",
    "            return last_shape, params\n",
    "\n",
    "        def eval_U(params, x, rng=None):\n",
    "            pstack = list(reversed(params))\n",
    "\n",
    "            def run(layer, x):\n",
    "                return layer[1](pstack.pop(), x)\n",
    "\n",
    "            (r, v) = x\n",
    "\n",
    "            r = run(rij_layer, r)\n",
    "            v = run(vij_layer, v)\n",
    "\n",
    "            last = run(vscale, v)\n",
    "            for i, att in enumerate(attentions):\n",
    "                residual_in = last\n",
    "                last = run(att, (r, last))\n",
    "                if self.arguments['block_nonlinearity']:\n",
    "                    last = run(block_nonlins[i], last)\n",
    "                if self.arguments['residual']:\n",
    "                    last = residual_in + last\n",
    "            last = run(final_attention, (r, last))\n",
    "            last = run(final_mlp, last)\n",
    "            return jnp.sum(last)\n",
    "\n",
    "        def eval_(params, x, rng=None):\n",
    "            result = jax.grad(eval_U, argnums=1)(params, x, rng)\n",
    "            # input is (r, v) so return grad wrt r\n",
    "            return result[0]\n",
    "\n",
    "        vscale = Dense(n_dim)\n",
    "        attentions = [make_attention() for _ in range(self.arguments['n_blocks'])]\n",
    "        block_nonlins = []\n",
    "        if self.arguments['block_nonlinearity']:\n",
    "            block_nonlins = self.arguments['n_blocks']*[value]\n",
    "        final_attention = make_attention(True)\n",
    "        final_mlp = serial(\n",
    "            Dense(dilation_dim), make_swish(), Dense(1))\n",
    "\n",
    "        scope['model_functions'] = init, eval_\n",
    "        scope['loss'] = 'mean_squared_error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import flowws\n",
    "from flowws import Argument as Arg\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "import jax.experimental.optimizers as optimizers\n",
    "\n",
    "OPTIMIZERS = dict(\n",
    "    adam=optimizers.adam,\n",
    "    sgd=optimizers.sgd\n",
    ")\n",
    "\n",
    "OPTIMIZER_ARGS = dict(\n",
    "    adam=[.005],\n",
    "    sgd=[.001]\n",
    ")\n",
    "\n",
    "class Losses:\n",
    "    @staticmethod\n",
    "    def sparse_categorical_crossentropy(prediction, y):\n",
    "        logp = prediction - logsumexp(prediction, axis=-1, keepdims=True)\n",
    "        return -jnp.take_along_axis(logp, y[..., None], axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def sparse_accuracy(prediction, y):\n",
    "        return jnp.argmax(prediction, axis=-1) == y\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(prediction, y):\n",
    "        return jnp.square(prediction - y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_absolute_error(prediction, y, y_scale=1., **kwargs):\n",
    "        return y_scale*jnp.abs(prediction - y)\n",
    "\n",
    "@flowws.add_stage_arguments\n",
    "class Train(flowws.Stage):\n",
    "    \"\"\"Train a jax model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ARGS = [\n",
    "        Arg('optimizer', '-o', str, 'adam',\n",
    "           help='optimizer to use'),\n",
    "        Arg('epochs', '-e', int, 32,\n",
    "           help='Max number of epochs'),\n",
    "        Arg('batch_size', '-b', int, 256,\n",
    "           help='Batch size'),\n",
    "        Arg('validation_split', '-v', float, .3),\n",
    "        Arg('seed', '-s', int, 13),\n",
    "        Arg('verbose', None, bool, True,\n",
    "            help='If True, print the training progress'),\n",
    "        Arg('metrics', None, [str], [],\n",
    "           help='Additional metrics to calculate'),\n",
    "        Arg('clip_gradients', None, float,\n",
    "           help='Clip gradients to the given magnitude')\n",
    "    ]\n",
    "\n",
    "    def run(self, scope, storage):\n",
    "        x_train, y_train = scope['x_train'], scope['y_train']\n",
    "        init_fun, eval_fun = scope['model_functions']\n",
    "\n",
    "        validation_data = None\n",
    "        if 'validation_data' in scope:\n",
    "            validation_data = scope['validation_data']\n",
    "        elif self.arguments['validation_split']:\n",
    "            if isinstance(x_train, (list, tuple)):\n",
    "                N = int(len(x_train[0])*self.arguments['validation_split'])\n",
    "                splits = [np.split(piece, [N]) for piece in x_train]\n",
    "                x_val = [piece[0] for piece in splits]\n",
    "                x_train = [piece[1] for piece in splits]\n",
    "            else:\n",
    "                N = int(len(x_train)*self.arguments['validation_split'])\n",
    "                x_val, x_train = np.split(x_train, [N])\n",
    "            y_val, y_train = np.split(y_train, [N])\n",
    "            validation_data = (x_val, y_val)\n",
    "\n",
    "        opt = self.arguments['optimizer']\n",
    "        (opt_init, opt_update, opt_params) = OPTIMIZERS[opt](*OPTIMIZER_ARGS[opt])\n",
    "\n",
    "        if isinstance(x_train, (list, tuple)):\n",
    "            x_shape = [v.shape for v in x_train]\n",
    "        else:\n",
    "            x_shape = x_train.shape\n",
    "\n",
    "        params = init_fun(jax.random.PRNGKey(self.arguments['seed']), x_shape)[1]\n",
    "        opt_state = opt_init(params)\n",
    "\n",
    "        lossfun = getattr(Losses, scope['loss'])\n",
    "\n",
    "        def loss(params, batch):\n",
    "            (x, y) = batch\n",
    "            prediction = eval_fun(params, x)\n",
    "            return jnp.sum(jnp.mean(lossfun(prediction, y), axis=0))\n",
    "\n",
    "        metric_names = list(scope.get('metrics', []))\n",
    "        metric_names.extend(self.arguments['metrics'])\n",
    "        @jax.jit\n",
    "        def metrics(params, batch):\n",
    "            (x, y) = batch\n",
    "            prediction = eval_fun(params, x)\n",
    "            result = []\n",
    "            for name in metric_names:\n",
    "                result.append(jnp.mean(getattr(Losses, name)(prediction, y, **scope)))\n",
    "            return jnp.array(result)\n",
    "\n",
    "        @jax.jit\n",
    "        def step(step, opt_state, batch):\n",
    "            params = opt_params(opt_state)\n",
    "            value, grads = jax.value_and_grad(loss)(params, batch)\n",
    "            if 'clip_gradients' in self.arguments:\n",
    "                val = self.arguments['clip_gradients']\n",
    "                grads = jax.tree_map(lambda v: jnp.clip(v, -val, val), grads)\n",
    "            opt_state = opt_update(step, grads, opt_state)\n",
    "            return value, opt_state, metrics(params, batch)\n",
    "\n",
    "        @jax.jit\n",
    "        def predict(params, x):\n",
    "            return jax.nn.softmax(eval_fun(params, x))\n",
    "\n",
    "        @jax.jit\n",
    "        def evaluate_batch(params, batch):\n",
    "            loss_val = loss(params, batch)\n",
    "            metric_vals = metrics(params, batch)\n",
    "            return jnp.concatenate([jnp.array([loss_val]), metric_vals])\n",
    "\n",
    "        def evaluate(params, batches):\n",
    "            return np.mean([evaluate_batch(params, batch_) for batch_ in batches], axis=0)\n",
    "\n",
    "        def batchfun(xs, ys):\n",
    "            batches = []\n",
    "            N = len(xs[0]) if isinstance(xs, (list, tuple)) else len(xs)\n",
    "            for i in range(0, N, self.arguments['batch_size']):\n",
    "                batch = slice(i, i + self.arguments['batch_size'])\n",
    "                if isinstance(xs, (list, tuple)):\n",
    "                    x = [piece[batch] for piece in xs]\n",
    "                else:\n",
    "                    x = xs[batch]\n",
    "                y = ys[batch]\n",
    "                batches.append((x, y))\n",
    "            return batches\n",
    "        batches = batchfun(x_train, y_train)\n",
    "        val_evaluate = functools.partial(evaluate, batches=batchfun(*validation_data))\n",
    "\n",
    "        step_count = 0\n",
    "        rng = np.random.default_rng(self.arguments['seed'])\n",
    "        batch_indices = np.arange(len(batches))\n",
    "        epoch_losses = []\n",
    "        for epoch in range(self.arguments['epochs']):\n",
    "            rng.shuffle(batch_indices)\n",
    "            batch_losses = []\n",
    "            for batch_index in batch_indices:\n",
    "                (last_loss, opt_state, batch_metrics) = step(step_count, opt_state, batches[batch_index])\n",
    "                batch_losses.append([last_loss] + list(batch_metrics))\n",
    "                step_count += 1\n",
    "            epoch_losses.append(np.mean(batch_losses, axis=0))\n",
    "            if validation_data is not None and metric_names:\n",
    "                val_evaluation = val_evaluate(opt_params(opt_state))\n",
    "                epoch_losses[-1] = np.concatenate([epoch_losses[-1], val_evaluation])\n",
    "            print(epoch, epoch_losses[-1])\n",
    "            batch_losses.clear()\n",
    "\n",
    "        scope['model'] = functools.partial(predict, opt_params(opt_state))\n",
    "        scope['train_log'] = epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowws\n",
    "from flowws_keras_geometry.data import PyriodicDataset\n",
    "\n",
    "w = flowws.Workflow(\n",
    "    [\n",
    "        MD17(cache_dir='/tmp', molecules=['toluene']),\n",
    "        ClearMetrics(),\n",
    "        MoleculeForceRegression(),\n",
    "        Train(epochs=40, batch_size=8, metrics=['mean_absolute_error'], clip_gradients=.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "scope = w.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "\n",
    "log = np.array(scope['train_log'])\n",
    "pp.plot(log[:, 0], label='Train')\n",
    "pp.plot(log[:, 2], label='Val')\n",
    "pp.xlabel('Epoch'); pp.ylabel('Loss')\n",
    "pp.gca().set_yscale('log')\n",
    "pp.legend()\n",
    "\n",
    "pp.figure()\n",
    "pp.plot(log[:, 1], label='Train')\n",
    "pp.plot(log[:, 3], label='Val')\n",
    "pp.xlabel('Epoch'); pp.ylabel('MAE')\n",
    "pp.gca().set_yscale('log')\n",
    "pp.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Molecular force regression using jax.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

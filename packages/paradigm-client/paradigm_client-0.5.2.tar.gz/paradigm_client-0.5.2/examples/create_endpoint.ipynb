{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7c8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "from packaging import version\n",
    "\n",
    "assert version.parse(python_version()) >= version.parse(\"3.10\"), \\\n",
    "    f\"It looks like you are using Python {python_version()} <3.10\" \\\n",
    "      \"paradigm_client requires a Python version>=3.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064c2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from paradigm_client.remote_model import RemoteModel\n",
    "\n",
    "api_key = os.environ.get(\"PARADIGM_API_KEY\", None)\n",
    "assert api_key is not None, \"{PARADIGM_API_KEY} env var is not properly set. Run `export PARADIGM_API_KEY=<value>` in your shell or add it to your `.bashrc`\"\n",
    "\n",
    "host_ip = os.environ[\"HOST\"]\n",
    "model = RemoteModel(host_ip, model_name=\"llm-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2a16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"Why is it sometimes difficult to communicate with large language models? Well, most of these models are pre-trained and have been exposed to a ton of human-written documents, such as books, blogs, code, and more. While they may have encountered some dialogue in these documents, they are not specifically trained to handle dialogue like a conversational agent.\n",
    "This means that asking them questions out of the blue may not always be the most effective way to communicate with them. But fear not! There are two alternative approaches that can help you get the most out of these models.\n",
    "\n",
    "Firstly, you can provide some context for the model by using question and answer tags. By doing this, you can help the model understand the context in which the question is being asked, which can improve the accuracy of its responses. Think of it like providing some background information before asking a question, so the model has a better idea of what you're asking.\n",
    "The second approach is to rephrase your queries to sound more like you're writing a document, rather than engaging in a conversation. This can help give the model more context to work with, which can help it generate more accurate and useful responses. So instead of asking \"What's the weather like today?\", you could say \"Can you provide information on today's weather conditions?\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd61575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are viable strategies to improve performance of large language models?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cfb07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Given the following document, answer to questions using **only** the information in the document.\n",
    "\n",
    "Document: \\\"{document}\\\"\n",
    "Question: \\\"{question}\\\"\n",
    "Answer:\\\"According to this document,\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad74676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"\\n\"]\n",
    "stop_regex = re.compile(r\"(?i)(\" + \"|\".join(re.escape(word) for word in stop_words) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20127acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.create(prompt, n_tokens=100, stop_regex=stop_regex, temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215de043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following document, answer to questions using **only** the information in the document.\n",
      "\n",
      "Document: \"Why is it sometimes difficult to communicate with large language models? Well, most of these models are pre-trained and have been exposed to a ton of human-written documents, such as books, blogs, code, and more. While they may have encountered some dialogue in these documents, they are not specifically trained to handle dialogue like a conversational agent.\n",
      "This means that asking them questions out of the blue may not always be the most effective way to communicate with them. But fear not! There are two alternative approaches that can help you get the most out of these models.\n",
      "\n",
      "Firstly, you can provide some context for the model by using question and answer tags. By doing this, you can help the model understand the context in which the question is being asked, which can improve the accuracy of its responses. Think of it like providing some background information before asking a question, so the model has a better idea of what you're asking.\n",
      "The second approach is to rephrase your queries to sound more like you're writing a document, rather than engaging in a conversation. This can help give the model more context to work with, which can help it generate more accurate and useful responses. So instead of asking \"What's the weather like today?\", you could say \"Can you provide information on today's weather conditions?\" \"\n",
      "Question: \"What are viable strategies to improve performance of large language models?\"\n",
      "Answer:\"According to this document,ðŸ¤– there are two ways to improve the performance of large language models. The first is to provide context for the model by using question and answer tags. The second is to rephrase your queries to sound more like you're writing a document, rather than engaging in a conversation.\n"
     ]
    }
   ],
   "source": [
    "print(response.input_text + \"ðŸ¤–\" + response.completions[0].output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9bcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

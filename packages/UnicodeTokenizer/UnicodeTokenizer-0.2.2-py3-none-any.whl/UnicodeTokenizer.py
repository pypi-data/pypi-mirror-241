# -*- coding: utf-8 -*-

from icu import BreakIterator, Locale

from tokenizers import pre_tokenizers


class UnicodeTokenizer:
    def __init__(self, lang="zh"):
        self.lang = lang
        self.word_breaker = BreakIterator.createWordInstance(Locale(lang))
        self.sentence_breaker = BreakIterator.createSentenceInstance(Locale(lang))
        self.pre_tokenizer = pre_tokenizers.Sequence([
            # pre_tokenizers.Whitespace(),  # \w+|[^\w\s]+
            # pre_tokenizers.UnicodeScripts(),
            pre_tokenizers.Punctuation("contiguous"),
            #  pre_tokenizers.Split(" ?[^(\\s|[.,!?â€¦ã€‚ï¼Œã€à¥¤Û”ØŒ])]+", "isolated"),
             pre_tokenizers.Split("\s+", "contiguous"),
              ])
        # self.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.UnicodeScripts()])

    def split_lines(self, text):
        self.sentence_breaker.setText(text)
        parts = []
        p0 = 0
        for p1 in self.sentence_breaker:
            part = text[p0:p1]
            parts.append(part)
            p0 = p1
        return parts

    def tokenize(self, text):
        tokens = []
        lines = self.split_lines(text)
        for line in lines:
            spans = self.pre_tokenizer.pre_tokenize_str(line)
            for span, bound in spans:
                tokens += self.tokenize_line(span)
        return tokens

    def tokenize_line(self, line):
        self.word_breaker.setText(line)
        parts = []
        p0 = 0
        for p1 in self.word_breaker:
            part = line[p0:p1]
            parts.append(part)
            p0 = p1
        return parts


def demo_token(line):
    tokenizer = UnicodeTokenizer()
    print(tokenizer.split_lines(line))
    print([word for word,(start,end) in tokenizer.pre_tokenizer.pre_tokenize_str(line)])
    print(tokenizer.tokenize_line(line))
    print(tokenizer.tokenize(line))


if __name__ == "__main__":
    line = """ ï¡¿                 
            é¦–å…ˆ8.88è®¾ç½® stã€‚art_new_word=True å’Œ output=[aÃ§aÃ­]ï¼Œoutput å°±æ˜¯æœ€ç»ˆï¡¿î´°Â‘ no such name"
            çš„è¾“å‡ºà¸„à¸¸à¸“à¸ˆà¸°à¸ˆà¸±à¸”à¸à¸´à¸˜à¸µà¹à¸•à¹ˆà¸‡à¸‡à¸²à¸™à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸£à¸„à¸°íƒ‘ìŠ¹ ìˆ˜ì†í•´ì•¼pneumonoultramicroscopicsilicovolcanoconiosis"
            í•˜ëŠ”ë° ì¹´ìš´í„°ê°€ ì–´ë””ì— ìˆì–´ìš”ê†ƒê­ê†ˆêŒ êŠ¨ê¦ê²ê…‰ê†…ê‰šê…‰ê‹ê‚·ê‚¶êŒ Ù„Ø£Ø­ÙŠØ§Ø¡ ØªÙ…Ø§Ø±ÙŠÙ† ØªØªØ·Ù„Ø¨ Ù…Ù† [MASK] [PAD] [CLS][SEP]
            est ğ—´‚ğ—¹­ğ˜œ¶ğ—´²ğ—‚§, ou "phiow-bjij-lhjij-lhjij", ce que l'on peut traduire par Â« pays-grand-blanc-Ã©levÃ© Â» (ç™½é«˜å¤§å¤åœ‹). 
        """.strip()
    # demo_token(line)
    l=line.splitlines()[0]
    demo_token(l)
        



"""
pre_tokenizers.Split(" ?[^(\\s|[.,!?â€¦ã€‚ï¼Œã€à¥¤Û”ØŒ])]+", "isolated").pre_tokenize_str(l)
"""
{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vL2VrLjtHZCs"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook helps to create and validate a training set for the k-NN classifier described in the MediaPipe [Pose Classification](https://google.github.io/mediapipe/solutions/pose_classification.html) solution, it has greatly been inspired by it code example. Test it on an arbitrary video, export to a CSV and then use it to develop your own application."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j8OxqytxxV-e"
      },
      "source": [
        "# Step 0: Env setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsA8WJi60PaX"
      },
      "outputs": [],
      "source": [
        "!pip install pillow matplotlib numpy opencv-python tqdm requests mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#use this if the \"Partie 2\" folder is not your work directory.\n",
        "%cd Partie\\ 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7S1Dl8Dhfa2"
      },
      "source": [
        "# Codebase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5Ay8WCLqosN"
      },
      "source": [
        "## Commons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swiAP0RYqqVM"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from PoseClassification.utils import show_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L6HLFd9AXmh"
      },
      "source": [
        "## Pose embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBrKOeP30RAx"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.pose_embedding import FullBodyPoseEmbedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH-efWS61Tfy"
      },
      "source": [
        "## Pose classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y230jVvP1u33"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.pose_classifier import PoseClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-VRo98tE1JH"
      },
      "source": [
        "## Classification smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POC4_eQsE3VO"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.utils import EMADictSmoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWuA2OYgGtZn"
      },
      "source": [
        "## Repetition counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEs_lgNiGv-j"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.utils import RepetitionCounter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBVyanN2Ic4W"
      },
      "source": [
        "## Classification visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgFLe1oTIgJH"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.visualize import PoseClassificationVisualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUbZ_c-Aq4B"
      },
      "source": [
        "## Bootstrap helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw2xYlGmAt3q"
      },
      "outputs": [],
      "source": [
        "from PoseClassification.bootstrap import BootstrapHelper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIiEj8Tx_x-q"
      },
      "source": [
        "# Step 1: Build classifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KpRszzilECFz"
      },
      "source": [
        "## Image samples\n",
        "\n",
        "Locally create a folder named `fitness_poses_images_in` with image samples.\n",
        "\n",
        "Images should repesent terminal states of desired pose classes. I.e. if you want to classify push-up provide iamges for two classes: when person is up, and when person is down.\n",
        "\n",
        "There should be about a few hundred samples per class covering different camera angles, environment conditions, body shapes, and exercise variations to build a good classifier.\n",
        "\n",
        "Required structure of the images_in_folder:\n",
        "```\n",
        "fitness_poses_images_in/\n",
        "  pushups_up/\n",
        "    image_001.jpg\n",
        "    image_002.jpg\n",
        "    ...\n",
        "  pushups_down/\n",
        "    image_001.jpg\n",
        "    image_002.jpg\n",
        "    ...\n",
        "  ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QBS_P_Y_2mg"
      },
      "source": [
        "## Bootstrap images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bERVPO8Ja6j7"
      },
      "outputs": [],
      "source": [
        "# Required structure of the images_in_folder:\n",
        "#\n",
        "#   fitness_poses_images_in/\n",
        "#     pushups_up/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     pushups_down/\n",
        "#       image_001.jpg\n",
        "#       image_002.jpg\n",
        "#       ...\n",
        "#     ...\n",
        "bootstrap_images_in_folder = 'fitness_poses_images_in'\n",
        "\n",
        "# Output folders for bootstrapped images and CSVs.\n",
        "bootstrap_images_out_folder = 'fitness_poses_images_out'\n",
        "bootstrap_csvs_out_folder = 'fitness_poses_csvs_out'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVYsbbJbOW7W"
      },
      "outputs": [],
      "source": [
        "# Initialize helper.\n",
        "bootstrap_helper = BootstrapHelper(\n",
        "    images_in_folder=bootstrap_images_in_folder,\n",
        "    images_out_folder=bootstrap_images_out_folder,\n",
        "    csvs_out_folder=bootstrap_csvs_out_folder,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e832H-X6b-v7"
      },
      "outputs": [],
      "source": [
        "# Check how many pose classes and images for them are available.\n",
        "bootstrap_helper.print_images_in_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAWtcZSHcQHc"
      },
      "outputs": [],
      "source": [
        "# Bootstrap all images.\n",
        "# Set limit to some small number for debug.\n",
        "bootstrap_helper.bootstrap(per_pose_class_limit=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRdqXeUScko9"
      },
      "outputs": [],
      "source": [
        "# Check how many images were bootstrapped.\n",
        "bootstrap_helper.print_images_out_statistics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTc3dlvFgg50"
      },
      "outputs": [],
      "source": [
        "# After initial bootstrapping images without detected poses were still saved in\n",
        "# the folderd (but not in the CSVs) for debug purpose. Let's remove them.\n",
        "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
        "bootstrap_helper.print_images_out_statistics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Sjl9JcJQzN"
      },
      "source": [
        "## Manual filtration\n",
        "\n",
        "Please manually verify predictions and remove samples (images) that has wrong pose prediction. Check as if you were asked to classify pose just from predicted landmarks. If you can't - remove it.\n",
        "\n",
        "Align CSVs and image folders once you are done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI7OytDZiHmg"
      },
      "outputs": [],
      "source": [
        "# Align CSVs with filtered images.\n",
        "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
        "bootstrap_helper.print_images_out_statistics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxcMoDLK8L0"
      },
      "source": [
        "## Automatic filtration\n",
        "\n",
        "Classify each sample against database of all other samples and check if it gets in the same class as annotated after classification.\n",
        "\n",
        "There can be two reasons for the outliers:\n",
        "\n",
        "  * **Wrong pose prediction**: In this case remove such outliers.\n",
        "\n",
        "  * **Wrong classification** (i.e. pose is predicted correctly and you aggree with original pose class assigned to the sample): In this case sample is from the underrepresented group (e.g. unusual angle or just very few samples). Add more similar samples and run bootstrapping from the very beginning.\n",
        "\n",
        "Even if you just removed some samples it makes sence to re-run automatic filtration one more time as database of poses has changed.\n",
        "\n",
        "**Important!!** Check that you are using the same parameters when classifying whole videos later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txnkReCiJr4Y"
      },
      "outputs": [],
      "source": [
        "# Find outliers.\n",
        "\n",
        "# Transforms pose landmarks into embedding.\n",
        "pose_embedder = FullBodyPoseEmbedding()\n",
        "\n",
        "# Classifies give pose against database of poses.\n",
        "pose_classifier = PoseClassifier(\n",
        "    pose_samples_folder=bootstrap_csvs_out_folder,\n",
        "    pose_embedder=pose_embedder,\n",
        "    top_n_by_max_distance=30,\n",
        "    top_n_by_mean_distance=10)\n",
        "\n",
        "outliers = pose_classifier.find_pose_sample_outliers()\n",
        "print('Number of outliers: ', len(outliers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS6TkprSklIw"
      },
      "outputs": [],
      "source": [
        "# Analyze outliers.\n",
        "bootstrap_helper.analyze_outliers(outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTWgo8-rlDFv"
      },
      "outputs": [],
      "source": [
        "# Remove all outliers (if you don't want to manually pick).\n",
        "bootstrap_helper.remove_outliers(outliers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LR9xOhPdlrkN"
      },
      "outputs": [],
      "source": [
        "# Align CSVs with images after removing outliers.\n",
        "bootstrap_helper.align_images_and_csvs(print_removed_items=False)\n",
        "bootstrap_helper.print_images_out_statistics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd0OSLWXMJFC"
      },
      "source": [
        "## Dump classifier data to CSV\n",
        "\n",
        "Dump filtered poses to CSV and download it.\n",
        "\n",
        "*Opt. : Please check this [guide](https://developers.google.com/ml-kit/vision/pose-detection/classifying-poses#4_integrate_with_the_ml_kit_quickstart_app) on how to use this CSV in the ML Kit sample app.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2VfIiLPML8A"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def dump_for_the_app():\n",
        "  pose_samples_folder = 'fitness_poses_csvs_out'\n",
        "  pose_samples_csv_path = 'fitness_poses_csvs_out.csv'\n",
        "  file_extension = 'csv'\n",
        "  file_separator = ','\n",
        "\n",
        "  # Each file in the folder represents one pose class.\n",
        "  file_names = [name for name in os.listdir(pose_samples_folder) if name.endswith(file_extension)]\n",
        "\n",
        "  with open(pose_samples_csv_path, 'w') as csv_out:\n",
        "    csv_out_writer = csv.writer(csv_out, delimiter=file_separator, quoting=csv.QUOTE_MINIMAL)\n",
        "    for file_name in file_names:\n",
        "      # Use file name as pose class name.\n",
        "      class_name = file_name[:-(len(file_extension) + 1)]\n",
        "\n",
        "      # One file line: `sample_00001,x1,y1,x2,y2,....`.\n",
        "      with open(os.path.join(pose_samples_folder, file_name)) as csv_in:\n",
        "        csv_in_reader = csv.reader(csv_in, delimiter=file_separator)\n",
        "        for row in csv_in_reader:\n",
        "          row.insert(1, class_name)\n",
        "          csv_out_writer.writerow(row)\n",
        "\n",
        "  # files.download(pose_samples_csv_path)\n",
        "\n",
        "\n",
        "dump_for_the_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfE7C9wHel_7"
      },
      "source": [
        "# Step 2: Classification\n",
        "\n",
        "**Important!!** Check that you are using the same classification parameters as while building classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYkZJ3a_2MW_"
      },
      "outputs": [],
      "source": [
        "# Specify your video name and target pose class to count the repetitions.\n",
        "video_path = 'pushups.mp4'\n",
        "class_name='pushups_down'\n",
        "out_video_path = 'pushups-sample-out.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3gGMDE0R2pe"
      },
      "outputs": [],
      "source": [
        "# Open the video.\n",
        "import cv2\n",
        "\n",
        "video_cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get some video parameters to generate output video with classificaiton.\n",
        "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
        "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "print(f\"video_n_frames: {video_n_frames}\")\n",
        "print(f\"video_fps: {video_fps}\")\n",
        "print(f\"video_width: {video_width}\")\n",
        "print(f\"video_height: {video_height}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t_ACEmTSOhr"
      },
      "outputs": [],
      "source": [
        "# Initialize tracker, classifier and counter.\n",
        "# Do that before every video as all of them have state.\n",
        "from mediapipe.python.solutions import pose as mp_pose\n",
        "\n",
        "\n",
        "# Folder with pose class CSVs. That should be the same folder you used while\n",
        "# building classifier to output CSVs.\n",
        "pose_samples_folder = 'fitness_poses_csvs_out'\n",
        "\n",
        "# Initialize tracker.\n",
        "pose_tracker = mp_pose.Pose()\n",
        "\n",
        "# Initialize embedder.\n",
        "pose_embedder = FullBodyPoseEmbedding()\n",
        "\n",
        "# Initialize classifier.\n",
        "# Check that you are using the same parameters as during bootstrapping.\n",
        "pose_classifier = PoseClassifier(\n",
        "    pose_samples_folder=pose_samples_folder,\n",
        "    pose_embedder=pose_embedder,\n",
        "    top_n_by_max_distance=30,\n",
        "    top_n_by_mean_distance=10)\n",
        "\n",
        "# # Uncomment to validate target poses used by classifier and find outliers.\n",
        "# outliers = pose_classifier.find_pose_sample_outliers()\n",
        "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
        "\n",
        "# Initialize EMA smoothing.\n",
        "pose_classification_filter = EMADictSmoothing(\n",
        "    window_size=10,\n",
        "    alpha=0.2)\n",
        "\n",
        "# Initialize counter.\n",
        "repetition_counter = RepetitionCounter(\n",
        "    class_name=class_name,\n",
        "    enter_threshold=6,\n",
        "    exit_threshold=4)\n",
        "\n",
        "# Initialize renderer.\n",
        "pose_classification_visualizer = PoseClassificationVisualizer(\n",
        "    class_name=class_name,\n",
        "    plot_x_max=video_n_frames,\n",
        "    # Graphic looks nicer if it's the same as `top_n_by_mean_distance`.\n",
        "    plot_y_max=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lXymkneOjgZ"
      },
      "outputs": [],
      "source": [
        "# Run classification on a video.\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from mediapipe.python.solutions import drawing_utils as mp_drawing\n",
        "\n",
        "\n",
        "# Open output video.\n",
        "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
        "\n",
        "frame_idx = 0\n",
        "output_frame = None\n",
        "with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:\n",
        "  while True:\n",
        "    # Get next frame of the video.\n",
        "    success, input_frame = video_cap.read()\n",
        "    if not success:\n",
        "      print(\"unable to read input video frame, breaking!\")\n",
        "      break\n",
        "\n",
        "    # Run pose tracker.\n",
        "    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
        "    result = pose_tracker.process(image=input_frame)\n",
        "    pose_landmarks = result.pose_landmarks\n",
        "\n",
        "    # Draw pose prediction.\n",
        "    output_frame = input_frame.copy()\n",
        "    if pose_landmarks is not None:\n",
        "      mp_drawing.draw_landmarks(\n",
        "          image=output_frame,\n",
        "          landmark_list=pose_landmarks,\n",
        "          connections=mp_pose.POSE_CONNECTIONS)\n",
        "    \n",
        "    if pose_landmarks is not None:\n",
        "      # Get landmarks.\n",
        "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
        "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
        "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
        "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
        "\n",
        "      # Classify the pose on the current frame.\n",
        "      pose_classification = pose_classifier(pose_landmarks)\n",
        "\n",
        "      # Smooth classification using EMA.\n",
        "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
        "\n",
        "      # Count repetitions.\n",
        "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
        "    else:\n",
        "      # No pose => no classification on current frame.\n",
        "      pose_classification = None\n",
        "\n",
        "      # Still add empty classification to the filter to maintaing correct\n",
        "      # smoothing for future frames.\n",
        "      pose_classification_filtered = pose_classification_filter(dict())\n",
        "      pose_classification_filtered = None\n",
        "\n",
        "      # Don't update the counter presuming that person is 'frozen'. Just\n",
        "      # take the latest repetitions count.\n",
        "      repetitions_count = repetition_counter.n_repeats\n",
        "\n",
        "    # Draw classification plot and repetition counter.\n",
        "    output_frame = pose_classification_visualizer(\n",
        "        frame=output_frame,\n",
        "        pose_classification=pose_classification,\n",
        "        pose_classification_filtered=pose_classification_filtered,\n",
        "        repetitions_count=repetitions_count)\n",
        "\n",
        "    # Save the output frame.\n",
        "    out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Show intermediate frames of the video to track progress.\n",
        "    if frame_idx % 50 == 0:\n",
        "      show_image(output_frame)\n",
        "\n",
        "    frame_idx += 1\n",
        "    pbar.update()\n",
        "\n",
        "# Close output video.\n",
        "out_video.release()\n",
        "\n",
        "# Release MediaPipe resources.\n",
        "# pose_tracker.close()\n",
        "\n",
        "# Show the last frame of the video.\n",
        "if output_frame is not None:\n",
        "  show_image(output_frame)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ACV-2023-1-90to_exh",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "931d4bdb822324d6d83c5eca646aed50afe39191627819df790863f2cfd01d4b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

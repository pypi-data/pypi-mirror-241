
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/model_building/plot_bcic_iv_2a_moabb_trial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_model_building_plot_bcic_iv_2a_moabb_trial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_model_building_plot_bcic_iv_2a_moabb_trial.py:


Basic Brain Decoding on EEG Data
========================================

This tutorial shows you how to train and test deep learning models with
Braindecode in a classical EEG setting: you have trials of data with
labels (e.g., Right Hand, Left Hand, etc.).

.. contents:: This example covers:
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 16-19

Loading and preparing the data
-------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 22-25

Loading the dataset
~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 28-38

First, we load the data. In this tutorial, we load the BCI Competition
IV 2a data [1]_ using braindecode's wrapper to load via
`MOABB library <https://github.com/NeuroTechX/moabb>`__ [2]_.

.. note::
   To load your own datasets either via mne or from
   preprocessed X/y numpy arrays, see `MNE Dataset
   Tutorial <./plot_mne_dataset_example.html>`__ and `Numpy Dataset
   Tutorial <./plot_custom_dataset_example.html>`__.


.. GENERATED FROM PYTHON SOURCE LINES 38-45

.. code-block:: default


    from braindecode.datasets import MOABBDataset

    subject_id = 3
    dataset = MOABBDataset(dataset_name="BNCI2014_001", subject_ids=[subject_id])









.. GENERATED FROM PYTHON SOURCE LINES 46-49

Preprocessing
~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 52-65

Now we apply preprocessing like bandpass filtering to our dataset. You
can either apply functions provided by
`mne.Raw <https://mne.tools/stable/generated/mne.io.Raw.html>`__ or
`mne.Epochs <https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs>`__
or apply your own functions, either to the MNE object or the underlying
numpy array.

.. note::
   Generally, braindecode prepocessing is directly applied to the loaded
   data, and not applied on-the-fly as transformations, such as in
   PyTorch-libraries like
   `torchvision <https://pytorch.org/docs/stable/torchvision/index.html>`__.


.. GENERATED FROM PYTHON SOURCE LINES 65-92

.. code-block:: default


    from numpy import multiply

    from braindecode.preprocessing import (Preprocessor,
                                           exponential_moving_standardize,
                                           preprocess)

    low_cut_hz = 4.  # low cut frequency for filtering
    high_cut_hz = 38.  # high cut frequency for filtering
    # Parameters for exponential moving standardization
    factor_new = 1e-3
    init_block_size = 1000
    # Factor to convert from V to uV
    factor = 1e6

    preprocessors = [
        Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors
        Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV
        Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter
        Preprocessor(exponential_moving_standardize,  # Exponential moving standardization
                     factor_new=factor_new, init_block_size=init_block_size)
    ]

    # Transform the data
    preprocess(dataset, preprocessors, n_jobs=-1)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/bru/PycharmProjects/braindecode-new/braindecode/preprocessing/preprocess.py:55: UserWarning: Preprocessing choices with lambda functions cannot be saved.
      warn('Preprocessing choices with lambda functions cannot be saved.')

    <braindecode.datasets.moabb.MOABBDataset object at 0x7f4215f85d30>



.. GENERATED FROM PYTHON SOURCE LINES 93-96

Extracting Compute Windows
~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 99-105

Now we extract compute windows from the signals, these will be the inputs
to the deep networks during training. In the case of trialwise
decoding, we just have to decide if we want to include some part
before and/or after the trial. For our work with this dataset,
it was often beneficial to also include the 500 ms before the trial.


.. GENERATED FROM PYTHON SOURCE LINES 105-125

.. code-block:: default


    from braindecode.preprocessing import create_windows_from_events

    trial_start_offset_seconds = -0.5
    # Extract sampling frequency, check that they are same in all datasets
    sfreq = dataset.datasets[0].raw.info['sfreq']
    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])
    # Calculate the trial start offset in samples.
    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)

    # Create windows using braindecode function for this. It needs parameters to define how
    # trials should be used.
    windows_dataset = create_windows_from_events(
        dataset,
        trial_start_offset_samples=trial_start_offset_samples,
        trial_stop_offset_samples=0,
        preload=True,
    )






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']
    Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']




.. GENERATED FROM PYTHON SOURCE LINES 126-129

Splitting the dataset into training and validation sets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 132-136

We can easily split the dataset using additional info stored in the
description attribute, in this case ``session`` column. We select
``T`` for training and ``test`` for validation.


.. GENERATED FROM PYTHON SOURCE LINES 136-142

.. code-block:: default


    splitted = windows_dataset.split('session')
    train_set = splitted['0train']  # Session train
    valid_set = splitted['1test']  # Session evaluation









.. GENERATED FROM PYTHON SOURCE LINES 143-146

Creating a model
------------


.. GENERATED FROM PYTHON SOURCE LINES 149-156

Now we create the deep learning model! Braindecode comes with some
predefined convolutional neural network architectures for raw
time-domain EEG. Here, we use the shallow ConvNet model from [3]_. These models are
pure `PyTorch <https://pytorch.org>`__ deep learning models, therefore
to use your own model, it just has to be a normal PyTorch
`nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__.


.. GENERATED FROM PYTHON SOURCE LINES 156-196

.. code-block:: default


    import torch

    from braindecode.models import ShallowFBCSPNet
    from braindecode.util import set_random_seeds

    cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it
    device = 'cuda' if cuda else 'cpu'
    if cuda:
        torch.backends.cudnn.benchmark = True
    # Set random seed to be able to roughly reproduce results
    # Note that with cudnn benchmark set to True, GPU indeterminism
    # may still make results substantially different between runs.
    # To obtain more consistent results at the cost of increased computation time,
    # you can set `cudnn_benchmark=False` in `set_random_seeds`
    # or remove `torch.backends.cudnn.benchmark = True`
    seed = 20200220
    set_random_seeds(seed=seed, cuda=cuda)

    n_classes = 4
    classes = list(range(n_classes))
    # Extract number of chans and time steps from dataset
    n_chans = train_set[0][0].shape[0]
    input_window_samples = train_set[0][0].shape[1]

    model = ShallowFBCSPNet(
        n_chans,
        n_classes,
        input_window_samples=input_window_samples,
        final_conv_length='auto',
    )

    # Display torchinfo table describing the model
    print(model)

    # Send model to GPU
    if cuda:
        model = model.cuda()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/bru/PycharmProjects/braindecode-new/braindecode/models/base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.
      warnings.warn(
    /home/bru/PycharmProjects/braindecode-new/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!
      warnings.warn("LogSoftmax final layer will be removed! " +
    ============================================================================================================================================
    Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape
    ============================================================================================================================================
    ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --
    ├─Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --
    ├─Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --
    ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --
    ├─BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --
    ├─Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --
    ├─AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]
    ├─Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
    ├─Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --
    ├─Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --
    │    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]
    │    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --
    │    └─Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --
    ============================================================================================================================================
    Total params: 47,364
    Trainable params: 47,364
    Non-trainable params: 0
    Total mult-adds (M): 0.01
    ============================================================================================================================================
    Input size (MB): 0.10
    Forward/backward pass size (MB): 0.35
    Params size (MB): 0.04
    Estimated Total Size (MB): 0.50
    ============================================================================================================================================




.. GENERATED FROM PYTHON SOURCE LINES 197-200

Model Training
--------------


.. GENERATED FROM PYTHON SOURCE LINES 203-208

Now we will train the network! ``EEGClassifier`` is a Braindecode object
responsible for managing the training of neural networks. It inherits
from skorch `NeuralNetClassifier <https://skorch.readthedocs.io/en/stable/classifier.html#>`__,
so the training logic is the same as in `Skorch <https://skorch.readthedocs.io/en/stable/>`__.


.. GENERATED FROM PYTHON SOURCE LINES 211-217

.. note::
   In this tutorial, we use some default parameters that we
   have found to work well for motor decoding, however we strongly
   encourage you to perform your own hyperparameter optimization using
   cross validation on your training data.


.. GENERATED FROM PYTHON SOURCE LINES 217-253

.. code-block:: default


    from skorch.callbacks import LRScheduler
    from skorch.helper import predefined_split

    from braindecode import EEGClassifier

    # We found these values to be good for the shallow network:
    lr = 0.0625 * 0.01
    weight_decay = 0

    # For deep4 they should be:
    # lr = 1 * 0.01
    # weight_decay = 0.5 * 0.001

    batch_size = 64
    n_epochs = 4

    clf = EEGClassifier(
        model,
        criterion=torch.nn.NLLLoss,
        optimizer=torch.optim.AdamW,
        train_split=predefined_split(valid_set),  # using valid_set for validation
        optimizer__lr=lr,
        optimizer__weight_decay=weight_decay,
        batch_size=batch_size,
        callbacks=[
            "accuracy", ("lr_scheduler", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),
        ],
        device=device,
        classes=classes,
    )
    # Model training for the specified number of epochs. `y` is None as it is
    # already supplied in the dataset.
    _ = clf.fit(train_set, y=None, epochs=n_epochs)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      epoch    train_accuracy    train_loss    valid_acc    valid_accuracy    valid_loss      lr     dur
    -------  ----------------  ------------  -----------  ----------------  ------------  ------  ------
          1            0.2500        1.5950       0.2535            0.2535        4.6092  0.0006  1.2582
          2            0.2708        1.2975       0.2535            0.2535        3.4426  0.0005  1.2589
          3            0.3333        1.2141       0.2674            0.2674        2.7940  0.0002  1.2030
          4            0.3438        1.1842       0.2778            0.2778        2.4347  0.0000  1.1932




.. GENERATED FROM PYTHON SOURCE LINES 254-257

Plotting Results
------------


.. GENERATED FROM PYTHON SOURCE LINES 260-263

Now we use the history stored by Skorch throughout training to plot
accuracy and loss curves.


.. GENERATED FROM PYTHON SOURCE LINES 263-301

.. code-block:: default


    import matplotlib.pyplot as plt
    import pandas as pd
    from matplotlib.lines import Line2D

    # Extract loss and accuracy values for plotting from history object
    results_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']
    df = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,
                      index=clf.history[:, 'epoch'])

    # get percent of misclass for better visual comparison to loss
    df = df.assign(train_misclass=100 - 100 * df.train_accuracy,
                   valid_misclass=100 - 100 * df.valid_accuracy)

    fig, ax1 = plt.subplots(figsize=(8, 3))
    df.loc[:, ['train_loss', 'valid_loss']].plot(
        ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)

    ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)
    ax1.set_ylabel("Loss", color='tab:blue', fontsize=14)

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

    df.loc[:, ['train_misclass', 'valid_misclass']].plot(
        ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)
    ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)
    ax2.set_ylabel("Misclassification Rate [%]", color='tab:red', fontsize=14)
    ax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend
    ax1.set_xlabel("Epoch", fontsize=14)

    # where some data has already been plotted to ax
    handles = []
    handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))
    handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))
    plt.legend(handles, [h.get_label() for h in handles], fontsize=14)
    plt.tight_layout()





.. image-sg:: /auto_examples/model_building/images/sphx_glr_plot_bcic_iv_2a_moabb_trial_001.png
   :alt: plot bcic iv 2a moabb trial
   :srcset: /auto_examples/model_building/images/sphx_glr_plot_bcic_iv_2a_moabb_trial_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 302-305

Plotting a  Confusion Matrix
----------------------------


.. GENERATED FROM PYTHON SOURCE LINES 308-310

Here we generate a confusion matrix as in [3]_.


.. GENERATED FROM PYTHON SOURCE LINES 310-333

.. code-block:: default



    from sklearn.metrics import confusion_matrix

    from braindecode.visualization import plot_confusion_matrix

    # generate confusion matrices
    # get the targets
    y_true = valid_set.get_metadata().target
    y_pred = clf.predict(valid_set)

    # generating confusion matrix
    confusion_mat = confusion_matrix(y_true, y_pred)

    # add class labels
    # label_dict is class_name : str -> i_class : int
    label_dict = windows_dataset.datasets[0].window_kwargs[0][1]['mapping']
    # sort the labels by values (values are integer class labels)
    labels = [k for k, v in sorted(label_dict.items(), key=lambda kv: kv[1])]

    # plot the basic conf. matrix
    plot_confusion_matrix(confusion_mat, class_names=labels)




.. image-sg:: /auto_examples/model_building/images/sphx_glr_plot_bcic_iv_2a_moabb_trial_002.png
   :alt: plot bcic iv 2a moabb trial
   :srcset: /auto_examples/model_building/images/sphx_glr_plot_bcic_iv_2a_moabb_trial_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <Figure size 640x480 with 1 Axes>



.. GENERATED FROM PYTHON SOURCE LINES 334-350

References
----------

.. [1] Tangermann, M., Müller, K.R., Aertsen, A., Birbaumer, N., Braun, C.,
       Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G.
       and Nolte, G., 2012. Review of the BCI competition IV.
       Frontiers in neuroscience, 6, p.55.

.. [2] Jayaram, Vinay, and Alexandre Barachant.
       "MOABB: trustworthy algorithm benchmarking for BCIs."
       Journal of neural engineering 15.6 (2018): 066011.

.. [3] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M.,
       Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W. and Ball, T. (2017),
       Deep learning with convolutional neural networks for EEG decoding and visualization.
       Hum. Brain Mapping, 38: 5391-5420. https://doi.org/10.1002/hbm.23730.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 12.360 seconds)

**Estimated memory usage:**  502 MB


.. _sphx_glr_download_auto_examples_model_building_plot_bcic_iv_2a_moabb_trial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_bcic_iv_2a_moabb_trial.py <plot_bcic_iv_2a_moabb_trial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_bcic_iv_2a_moabb_trial.ipynb <plot_bcic_iv_2a_moabb_trial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_

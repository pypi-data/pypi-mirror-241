Index: main_simple.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main_simple.ipynb b/main_simple.ipynb
--- a/main_simple.ipynb	(revision bde4c6343825e6a131547cdfdeed8a62c9ac4b11)
+++ b/main_simple.ipynb	(date 1700087094034)
@@ -184,13 +184,13 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m0 \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mdef\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;166;226;46;48;2;39;40;34mexecute_command\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m1 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimage_patch\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mImagePatch\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m2 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmuffin_patches\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimage_patch\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mmuffin\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m3 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkid_patches\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimage_patch\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mkid\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m4 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnum_muffins\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmuffin_patches\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                          \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m5 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnum_kids\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkid_patches\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                \u001b[0m\n",
-       "\u001b[1;38;2;227;227;221;48;2;39;40;34m  \u001b[0m\u001b[38;2;101;102;96;48;2;39;40;34m6 \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mreturn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmath\u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mceil\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnum_muffins\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;249;38;114;48;2;39;40;34m/\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnum_kids\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                   \u001b[0m\n"
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m0 \u001B[0m\u001B[38;2;102;217;239;48;2;39;40;34mdef\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;166;226;46;48;2;39;40;34mexecute_command\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mimage\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m:\u001B[0m\u001B[48;2;39;40;34m                                                                                    \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m1 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mimage_patch\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m=\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mImagePatch\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mimage\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                            \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m2 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mmuffin_patches\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m=\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mimage_patch\u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m.\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mfind\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34m\"\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34mmuffin\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34m\"\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m3 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mkid_patches\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m=\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mimage_patch\u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m.\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mfind\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34m\"\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34mkid\u001B[0m\u001B[38;2;230;219;116;48;2;39;40;34m\"\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                      \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m4 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mnum_muffins\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m=\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mlen\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mmuffin_patches\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                          \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m5 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mnum_kids\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m=\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mlen\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mkid_patches\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                                \u001B[0m\n",
+       "\u001B[1;38;2;227;227;221;48;2;39;40;34m  \u001B[0m\u001B[38;2;101;102;96;48;2;39;40;34m6 \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m    \u001B[0m\u001B[38;2;102;217;239;48;2;39;40;34mreturn\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mmath\u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m.\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mceil\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m(\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mnum_muffins\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;249;38;114;48;2;39;40;34m/\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m \u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34mnum_kids\u001B[0m\u001B[38;2;248;248;242;48;2;39;40;34m)\u001B[0m\u001B[48;2;39;40;34m                                                                   \u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -201,8 +201,7 @@
     "im = load_image('https://wondermamas.com/wp-content/uploads/2020/04/IMG_8950-min-1024x1024.jpg')\n",
     "query = 'How many muffins can each kid have for it to be fair?'\n",
     "\n",
-    "show_single_image(im)\n",
-    "code = get_code(query)"
+    "show_single_image(im)\n"
    ]
   },
   {
@@ -248,7 +247,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 1\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 1\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -284,7 +283,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 2\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 2\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -481,7 +480,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 3\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 3\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -540,7 +539,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 4\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 4\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -566,7 +565,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 5\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 5\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -592,7 +591,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m───────────────────────────────────────────────────── \u001b[0m\u001b[1mLine 6\u001b[0m\u001b[38;5;112m ──────────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m───────────────────────────────────────────────────── \u001B[0m\u001B[1mLine 6\u001B[0m\u001B[38;5;112m ──────────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
@@ -628,7 +627,7 @@
        "</pre>\n"
       ],
       "text/plain": [
-       "\u001b[38;5;112m────────────────────────────────────────────────── \u001b[0m\u001b[1mFinal Result\u001b[0m\u001b[38;5;112m ───────────────────────────────────────────────────\u001b[0m\n"
+       "\u001B[38;5;112m────────────────────────────────────────────────── \u001B[0m\u001B[1mFinal Result\u001B[0m\u001B[38;5;112m ───────────────────────────────────────────────────\u001B[0m\n"
       ]
      },
      "metadata": {},
Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
new file mode 100644
--- /dev/null	(date 1700087604898)
+++ b/main.py	(date 1700087604898)
@@ -0,0 +1,534 @@
+import ast
+import time
+
+import requests
+import queue
+import torch
+import torch.multiprocessing as mp
+import torchvision
+from rich.live import Live
+from rich.padding import Padding
+from rich.syntax import Syntax
+from rich import print
+from rich.markup import escape as rich_escape
+import inspect
+from typing import Callable, Union
+import dill
+
+from IPython.display import display
+from PIL import Image
+import matplotlib.pyplot as plt
+
+from IPython.core.display import HTML
+mp.set_start_method('spawn', force=True)
+
+
+# from image_patch import *
+# from video_segment import *
+
+CONFIG_MULTIPROCESSING = True
+CONFIG_LOAD_MODELS = True
+
+
+
+# if mp.current_process().name == 'MainProcess':
+#     # No need to initialize the models inside each process
+#     import vision_models
+#     # Create a list of all the defined models
+#     list_models = [m[1] for m in inspect.getmembers(vision_models, inspect.isclass)
+#                    if vision_models.BaseModel in m[1].__bases__]
+#     if CONFIG_MULTIPROCESSING:
+#         manager = mp.Manager()
+#     else:
+#         manager = None
+# else:
+list_models = None
+manager = None
+
+
+def make_fn(model_class, process_name, counter):
+    """
+    model_class.name and process_name will be the same unless the same model is used in multiple processes, for
+    different tasks
+    """
+    # We initialize each one on a separate GPU, to make sure there are no out of memory errors
+    num_gpus = torch.cuda.device_count()
+    gpu_number = counter % num_gpus
+
+    model_instance = model_class(gpu_number=gpu_number)
+
+    def _function(*args, **kwargs):
+        if process_name != model_class.name:
+            kwargs['process_name'] = process_name
+
+        if model_class.to_batch and not CONFIG_MULTIPROCESSING:
+            # Batchify the input. Model expects a batch. And later un-batchify the output.
+            args = [[arg] for arg in args]
+            kwargs = {k: [v] for k, v in kwargs.items()}
+
+            # The defaults that are not in args or kwargs, also need to listify
+            full_arg_spec = inspect.getfullargspec(model_instance.forward)
+            if full_arg_spec.defaults is None:
+                default_dict = {}
+            else:
+                default_dict = dict(zip(full_arg_spec.args[-len(full_arg_spec.defaults):], full_arg_spec.defaults))
+            non_given_args = full_arg_spec.args[1:][len(args):]
+            non_given_args = set(non_given_args) - set(kwargs.keys())
+            for arg_name in non_given_args:
+                kwargs[arg_name] = [default_dict[arg_name]]
+
+        try:
+            out = model_instance.forward(*args, **kwargs)
+            if model_class.to_batch and not CONFIG_MULTIPROCESSING:
+                out = out[0]
+        except Exception as e:
+            print(f'Error in {process_name} model:', e)
+            out = None
+        return out
+
+    return _function
+
+
+if CONFIG_MULTIPROCESSING:
+
+    def make_fn_process(model_class, process_name, counter):
+
+        if model_class.to_batch:
+            seconds_collect_data = model_class.seconds_collect_data  # Window of seconds to group inputs
+            max_batch_size = model_class.max_batch_size
+
+            def _function(queue_in):
+
+                fn = make_fn(model_class, process_name, counter)
+
+                to_end = False
+                while True:
+                    start_time = time()
+                    time_left = seconds_collect_data
+                    batch_inputs = []
+                    batch_queues = []
+                    while time_left > 0 and len(batch_inputs) < max_batch_size:
+                        try:
+                            received = queue_in.get(timeout=time_left)
+                            if received is None:
+                                to_end = True
+                                break
+                            else:
+                                batch_inputs.append(received[0])
+                                batch_queues.append(received[1])
+                        except queue.Empty:  # Time-out expired
+                            break  # Break inner loop (or do nothing, would break anyway because time_left < 0)
+                        time_left = seconds_collect_data - (time() - start_time)
+                    if len(batch_inputs) > 0:
+                        batch_kwargs = collate(batch_inputs, model_class.forward)
+                        outs = fn(**batch_kwargs)
+                        try:
+                            for out, qu in zip(outs, batch_queues):
+                                qu.put(out)
+                        except Exception as e:
+                            # No message, because we are just carrying the error from before
+                            for qu in batch_queues:
+                                qu.put(None)
+                    if to_end:
+                        print(f'{process_name} model exiting')
+                        break
+
+        else:
+            def _function(queue_in):
+                fn = make_fn(model_class, process_name, counter)
+                while True:
+                    received = queue_in.get()
+                    if received is None:
+                        print(f'{process_name} exiting')
+                        return
+                    (args, kwargs), queue_out = received
+                    out = fn(*args, **kwargs)
+                    queue_out.put(out)
+
+        return _function
+
+
+    if mp.current_process().name == 'MainProcess':
+        queues_in: Union[dict[str, mp.Queue], None] = dict()
+        consumers: dict[str, Union[mp.Process, Callable]] = dict()
+
+        counter_ = 0
+        for model_class_ in list_models:
+            for process_name_ in model_class_.list_processes():
+                # if process_name_ in config.load_models and config.load_models[process_name_]:
+                queue_in_ = manager.Queue()  # For transfer of data from producer to consumer
+                queues_in[process_name_] = queue_in_
+
+                fn_process = make_fn_process(model_class_, process_name_, counter_)
+                # Otherwise, it is not possible to pickle the _function (not defined at top level)
+                aux = mp.reducer.dump
+                mp.reducer.dump = dill.dump
+                consumer = mp.Process(target=fn_process, kwargs={'queue_in': queue_in_})
+                consumer.start()
+                mp.reducer.dump = aux
+                consumers[process_name_] = consumer
+
+                counter_ += 1
+
+    else:
+        queues_in = None
+
+
+    def finish_all_consumers():
+        # Wait for consumers to finish
+        for q_in in queues_in.values():
+            q_in.put(None)
+        for cons in consumers.values():
+            cons.join()
+
+else:
+
+    consumers = dict()
+
+    counter_ = 0
+    for model_class_ in list_models:
+        for process_name_ in model_class_.list_processes():
+            # if process_name_ in CONFIG_LOAD_MODELS and config.load_models[process_name_]:
+            consumers[process_name_] = make_fn(model_class_, process_name_, counter_)
+            counter_ += 1
+
+    queues_in = None
+
+    def finish_all_consumers():
+        pass
+
+
+def forward(model_name, *args, queues=None, **kwargs):
+    """
+    Sends data to consumer (calls their "forward" method), and returns the result
+    """
+    error_msg = f'No model named {model_name}. ' \
+                'The available models are: {}. Make sure to activate it in the configs files'
+    if not CONFIG_MULTIPROCESSING:
+        try:
+            out = consumers[model_name](*args, **kwargs)
+        except KeyError as e:
+            raise KeyError(error_msg.format(list(consumers.keys()))) from e
+    else:
+        if queues is None:
+            consumer_queues_in, queue_results = None, None
+        else:
+            consumer_queues_in, queue_results = queues
+        try:
+            if consumer_queues_in is not None:
+                consumer_queue_in = consumer_queues_in[model_name]
+            else:
+                consumer_queue_in = queues_in[model_name]
+        except KeyError as e:
+            options = list(consumer_queues_in.keys()) if consumer_queues_in is not None else list(queues_in.keys())
+            raise KeyError(error_msg.format(options)) from e
+        if queue_results is None:
+            # print('No queue exists to get results. Creating a new one, but this is inefficient. '
+            #       'Consider providing an existing queue for the process')
+            queue_results = manager.Queue()  # To get outputs
+        consumer_queue_in.put([(args, kwargs), queue_results])
+        out = queue_results.get()  # Wait for result
+    return out
+
+
+def collate(batch_inputs, fn):
+    """
+    Combine a list of inputs into a single dictionary. The dictionary contains all the parameters of the
+    function to be called. If the parameter is not defined in some samples, the default value is used. The
+    value of the parameters is always a list.
+    """
+    # Separate into args and kwargs
+    args_input, kwarg_input = list(zip(*batch_inputs))
+    full_arg_spec = inspect.getfullargspec(fn)
+    if full_arg_spec.defaults is None:
+        default_dict = {}
+    else:
+        default_dict = dict(zip(full_arg_spec.args[-len(full_arg_spec.defaults):], full_arg_spec.defaults))
+        if 'process_name' in default_dict:  # process_name is a special parameter filled in later
+            del default_dict['process_name']
+
+    args_list = full_arg_spec.args[1:]  # Remove self
+
+    # process_name is a special parameter filled in later
+    if 'process_name' in args_list:
+        assert args_list[-1] == 'process_name', 'process_name must be the last argument'
+        args_list.remove('process_name')
+
+    kwargs_output = {k: [] for k in args_list}
+    for i, (args, kwargs) in enumerate(zip(args_input, kwarg_input)):
+        if len(args) + len(kwargs) > len(args_list):
+            raise Exception(
+                f'You provided more arguments than the function {fn.__name__} accepts, or some kwargs/args '
+                f'overlap. The arguments are: {args_list}')
+        for j, arg_name in enumerate(args_list):
+            if len(args) > j:
+                kwargs_output[arg_name].append(args[j])
+            elif arg_name in kwargs:
+                kwargs_output[arg_name].append(kwargs[arg_name])
+            else:
+                assert arg_name in default_dict, f'You did not provide a value for the argument {arg_name}.'
+                kwargs_output[arg_name].append(default_dict[arg_name])
+
+    return kwargs_output
+
+
+
+# -----------------------------------------------------------------
+
+def inject_saver(code, show_intermediate_steps):
+    injected_function_name = 'show_all'
+    if injected_function_name in code:
+        return code
+    code = code.split("\n")
+    newcode = []
+    for n, codeline in enumerate(code):
+        codeline, indent = split_codeline_and_indent_level(codeline)
+
+        if codeline.startswith('#') or codeline == '':  # this will cause issues if you have lots of comment lines
+            continue
+        if '#' in codeline:
+            codeline = codeline.split('#')[0]
+
+        thing_to_show, code_type = get_thing_to_show_codetype(codeline)
+
+        if code_type in ('assign', 'append', 'if', 'return', 'for', 'sort', 'add'):
+            if '\'' in codeline:
+                codeline.replace('\'', '\\\'')
+
+            if show_intermediate_steps:
+                escape_thing = lambda x: x.replace("'", "\\'")
+                injection_string_format = \
+                    lambda \
+                        thing: f"{indent}{injected_function_name}(lineno={n},value=({thing}),valuename='{escape_thing(thing)}'," \
+                               f"fig=my_fig,time_wait_between_lines=0.5); " \
+                               f"CodexAtLine({n},syntax=syntax,time_wait_between_lines=0.5)"
+            else:
+                injection_string_format = lambda thing: f"{indent}CodexAtLine({n},syntax=syntax," \
+                                                        f"time_wait_between_lines=0.5)"
+
+            extension_list = []
+            if isinstance(thing_to_show, list):
+                injection_string_list = [injection_string_format(f"{thing}") for thing in thing_to_show]
+                extension_list.extend(injection_string_list)
+            elif code_type == 'for':
+                injection_string = injection_string_format(f"{thing_to_show}")
+                injection_string = " " * 4 + injection_string
+                extension_list.append(injection_string)
+            else:
+                extension_list.append(injection_string_format(f"{thing_to_show}"))
+
+            if code_type in ('if', 'return'):
+                extension_list = extension_list + [f"{indent}{codeline}"]
+            else:
+                extension_list = [f"{indent}{codeline}"] + extension_list
+
+            newcode.extend(extension_list)
+
+        elif code_type == 'elif_else':
+            newcode.append(f"{indent}{codeline}")
+        else:
+            newcode.append(f"{indent}{codeline}")
+    return "\n".join(newcode)
+
+
+def get_thing_to_show_codetype(codeline):
+    # can output either a list of things to show, or a single thing to show
+    things_to_show = []
+    if codeline.startswith("if"):
+        condition, rest = codeline[3:].split(":", 1)
+        codeline = f"if {condition}:{rest}"
+        code_type = "if"
+
+        operators = ['==', '!=', '>=', '<=', '>', '<']
+        things_to_show = []
+        for op in operators:
+            if op in condition:
+                things_to_show = [x.strip() for x in condition.split(op)]
+                # print(things_to_show)
+                break
+        # things_to_show.append(thing_to_show)
+        thing_to_show = things_to_show + [condition.strip()]
+
+    elif codeline.startswith("for"):
+        code_type = 'for'
+        thing_to_show = codeline.split("for ")[1].split(" in ")[0]
+
+    elif codeline.startswith("return"):
+        thing_to_show = codeline.split("return ")[1]
+        code_type = 'return'
+
+    elif ' = ' in codeline:
+        code_type = 'assign'
+        thing_to_show = codeline.split(' = ')[0]
+    elif ' += ' in codeline:
+        code_type = 'assign'
+        thing_to_show = codeline.split(' += ')[0]
+    elif ' -= ' in codeline:
+        code_type = 'assign'
+        thing_to_show = codeline.split(' -= ')[0]
+    elif ' *= ' in codeline:
+        code_type = 'assign'
+        thing_to_show = codeline.split(' *= ')[0]
+    elif ' /= ' in codeline:
+        code_type = 'assign'
+        thing_to_show = codeline.split(' /= ')[0]
+
+    elif '.append(' in codeline:
+        code_type = 'append'
+        thing_to_show = codeline.split('.append(')[0] + '[-1]'
+    elif '.add(' in codeline:
+        code_type = 'add'
+        thing_to_show = codeline.split('.add(')[0]
+
+    elif '.sort(' in codeline:
+        code_type = 'sort'
+        thing_to_show = codeline.split('.sort(')[0]
+
+    elif codeline.startswith("elif") or codeline.startswith("else"):
+        thing_to_show = None
+        code_type = 'elif_else'
+    else:
+        thing_to_show = None
+        code_type = 'other'
+
+    if isinstance(thing_to_show, list):
+        thing_to_show = [thing if not (thing.strip().startswith("'") and thing.strip().endswith("'"))
+                         else thing.replace("'", '"') for thing in thing_to_show if thing is not None]
+    elif isinstance(thing_to_show, str):
+        thing_to_show = thing_to_show if not (thing_to_show.strip().startswith("'") and
+                                              thing_to_show.strip().endswith("'")) else thing_to_show.replace("'", '"')
+    return thing_to_show, code_type
+
+
+def split_codeline_and_indent_level(codeline):
+    origlen = len(codeline)
+    codeline = codeline.lstrip()
+    indent = origlen - len(codeline)
+    indent = " " * indent
+    return codeline, indent
+
+
+# def show_one_image(image, ax):
+#     if isinstance(image, torch.Tensor):
+#         image = image.detach().cpu()
+#         if image.dtype == torch.float32:
+#             image = image.clamp(0, 1)
+#         image = image.squeeze(0).permute(1, 2, 0)
+#     ax.imshow(image)
+
+
+def CodexAtLine(lineno, syntax, time_wait_between_lines=1.):
+    syntax._stylized_ranges = []
+    syntax.stylize_range('on red', (lineno + 1, 0), (lineno + 1, 80))
+    time.sleep(time_wait_between_lines)
+
+
+# def show_all(lineno, value, valuename, fig=None, usefig=True, disp=True, time_wait_between_lines=None,
+#              lastlineno=[-1]):
+#     time.sleep(0.1)  # to avoid race condition!
+#
+#     thing_to_show = value
+#
+#     if lineno is not None and lineno != lastlineno[0]:
+#         lastlineno[0] = lineno  # ugly hack
+#
+#     if usefig:
+#         plt.clf()
+#         ax = fig.add_axes([0, 0, 1, 1])
+#         ax.set_xticks([])
+#         ax.set_yticks([])
+#     if isinstance(thing_to_show, Image.Image):
+#         show_one_image(thing_to_show, ax)
+#     elif str(type(thing_to_show)) == "<class 'image_patch.ImagePatch'>":
+#         show_one_image(thing_to_show.cropped_image, ax)
+#     elif isinstance(thing_to_show, list) or isinstance(thing_to_show, tuple):
+#         if len(thing_to_show) > 0:
+#             for i, thing in enumerate(thing_to_show):
+#                 disp_ = disp or i < len(thing_to_show) - 1
+#                 show_all(None, thing, f"{rich_escape(valuename)}[{i}]", fig=fig, disp=disp_, usefig=usefig)
+#             return
+#     elif isinstance(thing_to_show, dict):
+#         if len(thing_to_show) > 0:
+#             for i, (thing_k, thing_v) in enumerate(thing_to_show.items()):
+#                 disp_ = disp or i < len(thing_to_show) - 1
+#                 show_all(None, thing_v, f"{rich_escape(valuename)}['{thing_k}']", fig=fig, disp=disp_, usefig=usefig)
+#             return
+#     else:
+#         time.sleep(0.5 / 2)
+#         return
+#
+#     # display small
+#     if usefig:
+#         fig.set_size_inches(2, 2)
+#         if disp:
+#             display(fig)
+
+
+def load_image(path):
+    if path.startswith("http://") or path.startswith("https://"):
+        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')
+        image = torchvision.transforms.ToTensor()(image)
+    else:
+        image = Image.open(path)
+        image = torchvision.transforms.ToTensor()(image)
+    return image
+
+
+def get_code(query):
+    code = forward('codex', prompt=query, input_type="image")
+
+    # if config.codex.model not in ('gpt-3.5-turbo', 'gpt-4'):
+    code = f'def execute_command(image, my_fig, time_wait_between_lines, syntax):' + code
+    code = ast.unparse(ast.parse(code))
+    code_for_syntax_2 = code.replace("(image, my_fig, time_wait_between_lines, syntax)", "(image)")
+    syntax_2 = Syntax(code_for_syntax_2, "python", theme="monokai", line_numbers=True, start_line=0)
+    return code, syntax_2
+
+
+# def execute_code(code, im, show_intermediate_steps=True):
+#     code, syntax = code
+#     code_line = inject_saver(code, show_intermediate_steps, syntax, 0.5)
+#
+#     display(HTML("<style>.output_wrapper, .output {height:auto !important; max-height:1000000px;}</style>"))
+#
+#     with Live(Padding(syntax, 1), refresh_per_second=10, auto_refresh=True) as live:
+#         my_fig = plt.figure(figsize=(4, 4))
+#         try:
+#             exec(compile(code_line, 'Codex', 'exec'), globals())
+#             result = execute_command(im, my_fig, 0.5, syntax)  # The code is created in the exec()
+#         except Exception as e:
+#             print(f"Encountered error {e} when trying to run with visualizations. Trying from scratch.")
+#             exec(compile(code, 'Codex', 'exec'), globals())
+#             result = execute_command(im, my_fig, 0.5, syntax)  # The code is created in the exec()
+#
+#         plt.close(my_fig)
+#
+#     def is_not_fig(x):
+#         if x is None:
+#             return True
+#         elif isinstance(x, str):
+#             return True
+#         elif isinstance(x, float):
+#             return True
+#         elif isinstance(x, int):
+#             return True
+#         elif isinstance(x, list) or isinstance(x, tuple):
+#             return all([is_not_fig(xx) for xx in x])
+#         elif isinstance(x, dict):
+#             return all([is_not_fig(xx) for xx in x.values()])
+#         return False
+#
+#     f = None
+#     usefig = False
+#     if not is_not_fig(result):
+#         f = plt.figure(figsize=(4, 4))
+#         usefig = True
+#
+#     show_all(None, result, 'Result', fig=f, usefig=usefig, disp=False, time_wait_between_lines=0)
+
+
+def show_single_image(im):
+    im = Image.fromarray((im.detach().cpu().numpy().transpose(1, 2, 0) * 255).astype("uint8"))
+    im.copy()
+    im.thumbnail((400, 400))
+    display(im)

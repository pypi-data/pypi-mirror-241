config = {'best_match_model': 'xvlm',
 'blip_half_precision': True,
 'blip_v2_model_type': 'blip2-flan-t5-xl',
 'cached_codex_path': '',
 'clear_cache': False,
 'codex': {'best_of': 1,
           'max_tokens': 512,
           'model': 'gpt-3.5-turbo',
           'prompt': './prompts/chatapi.prompt',
           'temperature': 0.0},
 'crop_larger_margin': True,
 'dataset': {'batch_size': 20,
             'data_path': 'data',
             'max_samples': 100,
             'split': '',
             'start_sample': 0},
 'detect_thresholds': {'glip': 0.5, 'maskrcnn': 0.8, 'owlvit': 0.1},
 'execute_code': False,
 'fixed_code_file': './prompts/fixed_code/blip2.prompt',
 'gpt3': {'model': 'text-davinci-003',
          'n_votes': 1,
          'qa_prompt': './prompts/gpt3/gpt3_qa.txt',
          'temperature': 0.0},
 'load_models': {'blip': True,
                 'clip': False,
                 'codex': True,
                 'depth': True,
                 'glip': True,
                 'gpt3_general': True,
                 'gpt3_qa': True,
                 'maskrcnn': False,
                 'owlvit': False,
                 'saliency': False,
                 'tcl': False,
                 'xvlm': True},
 'log_every': 20,
 'multiprocessing': False,
 'path_pretrained_models': './pretrained_models',
 'ratio_box_area_to_image_area': 0.0,
 'results_dir': './results/',
 'save': True,
 'save_new_results': True,
 'use_cache': True,
 'use_cached_codex': False,
 'use_fixed_code': False,
 'verify_property': {'model': 'xvlm',
                     'thresh_clip': 0.6,
                     'thresh_tcl': 0.25,
                     'thresh_xvlm': 0.6},
 'wandb': False}


Metadata-Version: 2.1
Name: log10-io
Version: 0.4.5
Summary: Unified LLM data management
License: MIT
Author: log10 team
Requires-Python: >=3.9,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: anthropic (>=0.3.11,<0.4.0)
Requires-Dist: backoff (>=2.2.1,<3.0.0)
Requires-Dist: faiss-cpu (>=1.7.4,<2.0.0)
Requires-Dist: faker (>=19.2.0,<20.0.0)
Requires-Dist: google-cloud-bigquery (>=3.11.4,<4.0.0)
Requires-Dist: google-search-results (>=2.4.2,<3.0.0)
Requires-Dist: langchain (>=0.0.303,<0.0.304)
Requires-Dist: mosaicml-cli (>=0.5.30,<0.6.0)
Requires-Dist: openai (>=0.28.1,<0.29.0)
Requires-Dist: pexpect (>=4.8.0,<5.0.0)
Requires-Dist: python-dotenv (>=1.0.0,<2.0.0)
Requires-Dist: requests (>=2.31.0,<3.0.0)
Requires-Dist: tiktoken (>=0.4.0,<0.5.0)
Requires-Dist: together (>=0.2.7,<0.3.0)
Requires-Dist: wikipedia (>=1.4.0,<2.0.0)
Description-Content-Type: text/markdown

# log10

âš¡ Unified LLM data management âš¡

[![pypi](https://github.com/log10-io/log10/actions/workflows/release.yml/badge.svg)](https://github.com/log10-io/log10/actions/workflows/release.yml)
[![](https://dcbadge.vercel.app/api/server/CZQvnuRV94?compact=true&style=flat)](https://discord.gg/CZQvnuRV94)

## Quick Install

`pip install log10-io`

## ðŸ¤” What is this?

A one-line Python integration to manage your LLM data.

```python
import openai
from log10.load import log10

log10(openai)
# all your openai calls are now logged
```

Access your LLM data at [log10.io](https://log10.io)


## ðŸš€ What can this help with?

**ðŸ”ðŸž Prompt chain debugging**

Prompt chains such as those in [Langchain](https://github.com/hwchase17/langchain) can be difficult to debug. Log10 provides prompt provenance, session tracking and call stack functionality to help debug chains.

**ðŸ“ðŸ“Š Logging**

Log all your OpenAI calls to compare and find the best prompts, store feedback, collect latency and usage metrics, and perform analytics and compliance monitoring of LLM powered features.

You can log any openai (as [shown above](#ðŸ¤”-what-is-this)) or anthropic based application using the library wrappers from log10:

```python
import os
from log10.load import log10
import anthropic
import os

log10(anthropic)
anthropicClient = anthropic.Client()
# anthropic calls are now logged
```

This will log any LLM call through the process execution.

If you want to log other LLMs, you can use LangChain's LLM abstraction with the log10 logger:

```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage

from log10.langchain import Log10Callback
from log10.llm import Log10Config

log10_callback = Log10Callback(log10_config=Log10Config())

messages = [
    HumanMessage(content="You are a ping pong machine"),
    HumanMessage(content="Ping?"),
]

llm = ChatOpenAI(model_name="gpt-3.5-turbo", callbacks=[log10_callback])
```

Read more here for options for logging using library wrapper, langchain callback logger and how to apply log10 tags [here](./logging.md).

**ðŸ’¿ðŸ§© Flexible data store**

log10 provides a managed data store, but if you'd prefer to manage data in your own environment, you can use data stores like google big query.

Install the big query client library with:

`pip install log10-io[bigquery]`

And provide the following configuration in either a `.env` file, or as environment variables:

| Name | Description |
|------|-------------|
| `LOG10_DATA_STORE`  |  Either `log10` or `bigquery` |
| `LOG10_BQ_PROJECT_ID`   | Your google cloud project id      |
| `LOG10_BQ_DATASET_ID`  |  The big query dataset id  |
| `LOG10_BQ_COMPLETIONS_TABLE_ID` | The name of the table to store completions in |

**Note** that your environment should have been setup with google cloud credentials. Read more [here](https://cloud.google.com/sdk/gcloud/reference/auth/login) about authenticating.

**ðŸ§ ðŸ” Readiness for RLHF & self hosting**

Use your data and feedback from users to fine-tune custom models with RLHF with the option of building and deploying more reliable, accurate and efficient self-hosted models. 

**ðŸ‘¥ðŸ¤ Collaboration**

Create flexible groups to share and collaborate over all of the above features

## âš™ï¸ Setup

1. Create a free account at [log10.io](https://log10.io)
2. Set the following environment variables:
- `LOG10_URL=https://log10.io`
- `LOG10_TOKEN`: From the Settings tab in log10.io
- `LOG10_ORG_ID`: From the Organization tab in log10.io
- `OPENAI_API_KEY`: OpenAI API key
- `ANTHROPIC_API_KEY`: Anthropic API key

### âœ… Run examples and tests

You can find and run examples under folder `examples`, e.g. run a logging example:
```
python examples/logging/chatcompletion.py
``` 

Also you can run some end-to-end tests with [`xdocttest`](https://github.com/Erotemic/xdoctest) installed (`pip install xdoctest`).

```
# list all tests
python -m xdoctest log10 list

# run all tests
python -m xdoctest log10 all

# run a single test, e.g.
python -m xdoctest /Users/wenzhe/dev/log10/log10/load.py log10:0
```

## ðŸ’¬ Community

We welcome community participation and feedback. Please leave an issue, submit a PR or join our [Discord](https://discord.gg/CZQvnuRV94).

